{"0": {
    "doc": "About2",
    "title": "About2",
    "content": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. ",
    "url": "http://localhost:4000/_posts",
    "relUrl": "/_posts"
  },"1": {
    "doc": "child page",
    "title": "this is a test",
    "content": " ",
    "url": "http://localhost:4000/child1.html#this-is-a-test",
    "relUrl": "/child1.html#this-is-a-test"
  },"2": {
    "doc": "child page",
    "title": "this is a test",
    "content": "this is a test . Bold text test . testing . | first | second | third1. first | second | third1. first | second | third1. first | second | third1. first | second | third1. first | second | third1. first | second | third1. first | second | third1. first | second | third1. first | second | third1. first | second | third1. first | second | third1. first | second | third1. first | second | third | . ",
    "url": "http://localhost:4000/child1.html#this-is-a-test-1",
    "relUrl": "/child1.html#this-is-a-test-1"
  },"3": {
    "doc": "child page",
    "title": "haskjfd",
    "content": ". | First | Second | Third | . int e = 11; // this is a test . this is a test . this is a test . # this is a test int i = 0; . ",
    "url": "http://localhost:4000/child1.html#haskjfd",
    "relUrl": "/child1.html#haskjfd"
  },"4": {
    "doc": "child page",
    "title": "child page",
    "content": "testing this shit . ",
    "url": "http://localhost:4000/child1.html",
    "relUrl": "/child1.html"
  },"5": {
    "doc": "Home",
    "title": "Home",
    "content": "In this Documentation we are going to go through the ELK stack and try to understand the stack elements and there configuration. Overview of the stack . Installation and configuration . Usage . Resources . ",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },"6": {
    "doc": "Installation",
    "title": "Elasticsearch installation and configuration on Linux",
    "content": "To download Elasticsearch archive we can use wget or curl: . wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.3.1-linux-x86_64.tar.gz tar -xzf elasticsearch-8.3.1-linux-x86_64.tar.gz cd elasticsearch-8.3.1/ . Your main config file is located at elasticsearch-8.3.1/config/elasticsearch.yml . If you installed elasticsearch using apt or dpkg your config file witll be located at /etc/elasticsearch/elasticsearch.yml . The archive contain a great configuration sample, it is minimal and contain all the configuration to start. Let’s try to understand the most important ones. # ----------------------------------- Cluster ----------------------------------- #The cluster name is straight forward. it is the name of your cluster. cluster.name: \"myfirst-cluster\" # ------------------------------------ Node ------------------------------------ # Use a descriptive name for the node: node.name: \"master-node\" # ----------------------------------- Paths ------------------------------------ # Path to directory where to store the data, Or you can leave it and elasticsearch will store the data at the default location with is elasticsearch-8.3.1/data/ #path.data: /path/to/data # Path to log files. Same thing we can leave it and the default log location will be elasticsearch-8.3.1/logs/ #path.logs: /path/to/log #Note: If you are running multple nodes from the same directory you may specify each node log and data location to avoid conflictions. # ---------------------------------- Network ----------------------------------- # By default Elasticsearch is only accessible on localhost. Set a different # address here to expose this node on the network: network.host: 0.0.0.0 # By default Elasticsearch listens for HTTP traffic on the first free port it # finds starting at 9200. Set a specific HTTP port here: http.port: 9200 # --------------------------------- Discovery ---------------------------------- # # Discovery seed hosts are used when your nodes are hosted in diffrent hosts. If you want to run multiple nodes in the same host leave it disabled and the default seed_hosts will be set to 127.0.0.1. #discovery.seed_hosts: [\"host1\", \"host2\"] # If you are trying to run a single-node set the discovery type to single-node discovery.type: single-node # Bootstrap the cluster using an initial set of master-eligible nodes: #cluster.initial_master_nodes: [\"node-1\", \"node-2\"] . After configuring elasticsearch we can start it: ./bin/elasticsearch . As you can see we started elasticsearch succesfuly and it provided us with the password for elastic user and two enrollment tokens we can use them for enrolling kibana or other nodes to out cluster. Elasticsearch also provides multiple binaries to reset passwords or create enrollment tokens. ",
    "url": "http://localhost:4000/installation/#elasticsearch-installation-and-configuration-on-linux",
    "relUrl": "/installation/#elasticsearch-installation-and-configuration-on-linux"
  },"7": {
    "doc": "Installation",
    "title": "Running multple nodes on the same host",
    "content": "To run multiple nodes in the same host we need to reconfigure our master-node. Resource#1 . For Elasticsearch clusters, ECE gives 50% of the available memory to the JVM heap used by Elasticsearch, while leaving the other 50% for the operating system. This memory won’t go unused, as Lucene is designed to leverage the underlying OS for caching in-memory data structures, meaning that Lucene will happily gobble up whatever is left over. The ideal heap size is somewhere below 32 GB, as heap sizes above 32 GB become less efficient. What these recommendations mean is that on a 64 GB cluster, we dedicate 32 GB to the Elasticsearch heap and 32 GB to the operating system in the container that hosts your cluster. If you provision a 128 GB cluster, we create two 64 GB nodes, each node with 32 GB reserved for the Elasticsearch heap and 32 GB reserved for the operating system. To not exceed the memory limit we can set the ES_JAVA_OPTS envirment variable and specify the min and max memory usage . export ES_JAVA_OPTS=\"-Xms2g -Xmx2g\" . For the master node we gonna use the same config as before, and change these parameters: . # Set the discovery to multiple nodes discovery.type: multi-node # specify the data path path.data: data-master-node # specify the logs path path.logs: logs-master-node . Before running the master node we gonna generate out ssl certs to enable secure connection between nodes . # lets generate our own CA ./bin/elasticsearch-certutil ca --pem # after that lets generate out cert and sign it using the CA ./bin/elasticsearch-certutil cert --pem --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key . After generating the certificate we can configure ssl and tls in out elasticsearch.yml . xpack.security.enabled: true xpack.security.http.ssl.enabled: true xpack.security.http.ssl.key: certs/instance/instance.key xpack.security.http.ssl.certificate: certs/instance/instance.crt xpack.security.http.ssl.certificate_authorities: certs/ca/ca.crt xpack.security.http.ssl.verification_mode: certificate xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.key: certs/instance/instance.key xpack.security.transport.ssl.certificate: certs/instance/instance.crt xpack.security.transport.ssl.certificate_authorities: certs/ca/ca.crt xpack.security.transport.ssl.verification_mode: certificate . After that we can run the master node as before: ./bin/elasticsearch . For the second node we gonna change these configs: . # change node name node.name: \"second-node\" # change data path path.data: data-second-node # change logs path path.logs: logs-second-node # set ssl ca to enable secure connection between nodes xpack.security.enabled: true xpack.security.http.ssl.enabled: true xpack.security.http.ssl.key: certs/instance/instance.key xpack.security.http.ssl.certificate: certs/instance/instance.crt xpack.security.http.ssl.certificate_authorities: certs/ca/ca.crt xpack.security.http.ssl.verification_mode: certificate xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.key: certs/instance/instance.key xpack.security.transport.ssl.certificate: certs/instance/instance.crt xpack.security.transport.ssl.certificate_authorities: certs/ca/ca.crt xpack.security.transport.ssl.verification_mode: certificate . Before running the second node we need to generate the enrollment token ./bin/elasticsearch-create-enrollment-token -s node . -s flag stand for scope you can either generate an enrollment token for kibana or for another node . now lets run the second node ./bin/elasticsearch --enrollment-token eyJ2ZXIiOiI4LjMuMSIsImFkciI6WyIxMC4xODIuMC4xMTo5MjAwIl0sImZnciI6IjYzMmE0NjIxZTYyYTE0ODAwNDkwYzk0MGMyOGZlNGUwZDdlOWY0YTk4NzQ1NDY1YjNkNGM5ODhkYzdhZGY4NjQiLCJrZXkiOiIzM0stQW9JQmRsWWMxeGxOaUNpYjpjUFlMd01wY1E4T010LVAyLWt0MmlBIn0= . We can see that both nodes are running and the second node succesfuly enrolled into out cluster. To run elasticsearch as daemon you can either use -d flag or install elasticsearch using apt./bin/elasticsearch -d . ",
    "url": "http://localhost:4000/installation/#running-multple-nodes-on-the-same-host",
    "relUrl": "/installation/#running-multple-nodes-on-the-same-host"
  },"8": {
    "doc": "Installation",
    "title": "Installing Logstash",
    "content": "Download logstash archive: . wget https://artifacts.elastic.co/downloads/logstash/logstash-8.3.1-linux-x86_64.tar.gz tar -xzf logstash-8.3.1-linux-x86_64.tar.gz cd logstash-8.3.1-linux-x86_64 . The configuration file for logstash is located at logstash-8.3.1-linux-x86_64/config/logstash.yml . Let’s configure logstash . # Changing host to be able to access logstash externaly http.host: \"0.0.0.0\" http.port: 9600 # The pipeline.workers setting determines how many threads to run for filter and output processing. pipeline: workers: 2 node: name: \"master-node\" . We can configure logstash input and output pipelines using logstash.conf file. We gonna use a CSV file for testing . input { file { path =&gt; \"/root/logstash-8.3.1/config/Games.csv\" start_position =&gt; [\"beginning\"] sincedb_path =&gt; \"NULL\" } } filter { csv { separator =&gt; \",\" columns =&gt; [ \"Name\", \"Sales\", \"Series\", \"Release\", \"Genre\" , \"Developer\", \"Publisher\" ] } } output { elasticsearch { hosts =&gt; [\"https://localhost:9200\"] # Specify the ca cert to trust elasticsearch cert cacert =&gt; \"/root/config/certs/ca/ca.crt\" # Specify index name index =&gt; \"games\" document_type =&gt; \"sold_games\" # Specify the credential of the user used to authenticat to elasticsearch user =&gt; \"elastic\" password =&gt; \"elasticpassword\" } # stdout is a great way to debug the ouput that are gonna be sent to elasticsearch stdout {} } . After that lets start logstash ./bin/logstash -f config/logstash.conf . All the data is stored on elasticsearch and we can comfirm that after installing kibana. ",
    "url": "http://localhost:4000/installation/#installing-logstash",
    "relUrl": "/installation/#installing-logstash"
  },"9": {
    "doc": "Installation",
    "title": "Kibana installation",
    "content": "As elasticsearch and logstash we can download the archive containing the binaries directly or we can use any package manager (Depending on your distro of choice) to install kibana. wget https://artifacts.elastic.co/downloads/kibana/kibana-8.3.1-linux-x86_64.tar.gz tar -xzf kibana-8.3.1-linux-x86_64.tar.gz cd kibana-8.3.1/ . This is the important configs we need to start kibana: . # Changing host to be able to access kibana externaly server.host: \"0.0.0.0\" # Provide the host and port for your elasticsearch cluster elasticsearch.hosts: [\"https://localhost:9200\"] # Provide the crendentials to your kibana_system user, you can reset the password for this user using elasticsearch-reset-password binary. elasticsearch.username: \"kibana_system\" elasticsearch.password: kibana_password # Provide kibana with the your CA certificate so the connection can be verified between kibana and elasticsearch elasticsearch.ssl.verificationMode: certificate elasticsearch.ssl.certificateAuthorities: [ \"/etc/kibana/config/certs/ca/ca.crt\" ] . To run kibana: ./bin/kibana . After running kibana we can access it at http://localhost:5601 . Lets confirm that logstash data is stored by checking Analytics-&gt;Discovery . First we need to cearte a data view for the index we used in our data . ",
    "url": "http://localhost:4000/installation/#kibana-installation",
    "relUrl": "/installation/#kibana-installation"
  },"10": {
    "doc": "Installation",
    "title": "Monitor a windows machine using sysmon logs",
    "content": ". | Download Sysmon | . https://download.sysinternals.com/files/SysinternalsSuite.zip . | We are going to use existing configuration by olafhartong | . https://github.com/olafhartong/sysmon-modular . https://raw.githubusercontent.com/olafhartong/sysmon-modular/master/sysmonconfig.xml . | Install by opening up a command prompt as administrator | . # 32 bit sysmon.exe –accepteula –i PATH/TO/sysmonconfig.xml # 64 bit sysmon64.exe –accepteula –i PATH/TO/sysmonconfig.xml . | Download Winlogbeat | . https://artifacts.elastic.co/downloads/beats/winlogbeat/winlogbeat-8.3.2-windows-x86_64.zip . | Extract the contents into C:\\Program Files. | Rename the winlogbeat- directory to Winlogbeat . | Using PowerShell prompt as an Administrator . | . # Access winlogbeat directory cd 'C:\\Program Files\\Winlogbeat' # Install winlogbeat service .\\install-service-winlogbeat.ps1 . | Configure winlogbeat to connect ro the elastic stack | . winlogbeat.yml . output.elasticsearch: hosts: [\"https://YOUR-HOST:9200\"] username: \"winlogbeat_internal\" password: \"YOUR_PASSWORD\" . # provide kibana host credential to setup dashboards setup.kibana: host: \"mykibanahost:5601\" username: \"my_kibana_user\" password: \"{pwd}\" . | Setup assets | .\\winlogbeat.exe setup -e . | Start Winlogbeat | . Start-Service winlogbeat . You can check the Discover section to confirm data is shipping succesfully. ",
    "url": "http://localhost:4000/installation/#monitor-a-windows-machine-using-sysmon-logs",
    "relUrl": "/installation/#monitor-a-windows-machine-using-sysmon-logs"
  },"11": {
    "doc": "Installation",
    "title": "Installation",
    "content": "All the configurations in this documantation are for version 8.3.1 of the stack. All the configuration files are included in the main branch of this repo. Note: all the elk stack software we installed are the same version, Be carefull with the version you choose to install because elk stack does not do well with back compatibility. Support Matrix for all Elastic softwares . ",
    "url": "http://localhost:4000/installation/",
    "relUrl": "/installation/"
  },"12": {
    "doc": "Overview",
    "title": "Elasticsearch",
    "content": "Elasticsearch is a search and analytics engine. It is based on Lucene search engine, and it is built with RESTful APIS. It offers simple deployment, maximum reliability, and easy management. It also offers advanced queries to perform detail analysis and stores all the data centrally. It is helpful for executing a quick search of the documents. ",
    "url": "http://localhost:4000/Overview/#elasticsearch",
    "relUrl": "/Overview/#elasticsearch"
  },"13": {
    "doc": "Overview",
    "title": "Features of Elasticsearch",
    "content": ". | Near real-time searching: | . Elasticsearch is a distributed document store. Instead of storing information as rows of columnar data, Elasticsearch stores complex data structures that have been serialized as JSON documents. When you have multiple Elasticsearch nodes in a cluster, stored documents are distributed across the cluster and can be accessed immediately from any node. When a document is stored, it is indexed and fully searchable in near real-time–within 1 second. Elasticsearch uses a data structure called an inverted index that supports very fast full-text searches. An inverted index lists every unique word that appears in any document and identifies all of the documents each word occurs in. | High level of customization: | . Managing indexing lifecycle . Resource#1 . Depending on your need you can customize the indexing lifecycle, per example you can configure elasticsearch to automaticlly create a new indices when an index reaches a certain size. Node roles . Resource#1 Resource#2 . You can assign a role to your nodes, to enssure efficancy out of your cluster. By creating a collection of nodes with the same data role you are creating data tiers that are the defined as follows . | Content tier nodes handle the indexing and query load for content such as a product catalog. | Hot tier nodes handle the indexing load for time series data such as logs or metrics and hold your most recent, most-frequently-accessed data. | Warm tier nodes hold time series data that is accessed less-frequently and rarely needs to be updated. | Cold tier nodes hold time series data that is accessed infrequently and not normally updated. To save space, you can keep fully mounted indices of searchable snapshots on the cold tier. These fully mounted indices eliminate the need for replicas, reducing required disk space by approximately 50% compared to the regular indices. | Frozen tier nodes hold time series data that is accessed rarely and never updated. The frozen tier stores partially mounted indices of searchable snapshots exclusively. This extends the storage capacity even further — by up to 20 times compared to the warm tier. | . RESTful APIS . Resource#1 . Elasticsearch provide us with an extensive RESTful APIs which allow us to integrate, manage and query the indexed data without using an interface. Scalability, Resilience and Redundancy . Resource#1 . Elasticsearch is built to be always available and to scale with your needs. It does this by being distributed by nature. You can add nodes to a cluster to increase capacity and Elasticsearch automatically distributes your data and query load across all of the available nodes. No need to overhaul your application, Elasticsearch knows how to balance multi-node clusters to provide scale and high availability. By distributing the documents in an index across multiple shards, and distributing those shards across multiple nodes, Elasticsearch can ensure redundancy, which both protects against hardware failures and increases query capacity as nodes are added to a cluster. ",
    "url": "http://localhost:4000/Overview/#features-of-elasticsearch",
    "relUrl": "/Overview/#features-of-elasticsearch"
  },"14": {
    "doc": "Overview",
    "title": "Logstash",
    "content": "Logstash is a plugin-based data collection and processing engine. It comes with a wide range of plugins that makes it possible to easily configre it to collect, process and forward data in many different architectures. Processing is organized into one or more pipelines. In each pipeline, one or more input plugins receive or collect data that is then placed on an internal queue. This is by default small and held in memory, but can be configured to be larger and persisted on disk in order to improve reliability and resiliency. ",
    "url": "http://localhost:4000/Overview/#logstash",
    "relUrl": "/Overview/#logstash"
  },"15": {
    "doc": "Overview",
    "title": "Kibana",
    "content": "Kibana is a data visualization and exploration tool used for log and time-series analytics, application monitoring, and operational intelligence use cases. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support. ",
    "url": "http://localhost:4000/Overview/#kibana",
    "relUrl": "/Overview/#kibana"
  },"16": {
    "doc": "Overview",
    "title": "",
    "content": " ",
    "url": "http://localhost:4000/Overview/",
    "relUrl": "/Overview/"
  },"17": {
    "doc": "Overview",
    "title": "Overview",
    "content": "So, what is the ELK Stack? “ELK” is the acronym for three open source projects: Elasticsearch, Logstash, and Kibana. ",
    "url": "http://localhost:4000/Overview/",
    "relUrl": "/Overview/"
  },"18": {
    "doc": "Resources",
    "title": "What is a SIEM",
    "content": "https://www.ibm.com/topics/siem . SIEM tools are an important part of the data security ecosystem: they aggregate data from multiple systems and analyze that data to catch abnormal behavior or potential cyberattacks. SIEM tools provide a central place to collect events and alerts – but can be expensive, resource intensive, and customers report that it is often difficult to resolve problems with SIEM data. ",
    "url": "http://localhost:4000/Resources#what-is-a-siem",
    "relUrl": "/Resources#what-is-a-siem"
  },"19": {
    "doc": "Resources",
    "title": "Elk stack as a SIEM",
    "content": "https://www.elastic.co/security/siem . ",
    "url": "http://localhost:4000/Resources#elk-stack-as-a-siem",
    "relUrl": "/Resources#elk-stack-as-a-siem"
  },"20": {
    "doc": "Resources",
    "title": "Elk stack architecture",
    "content": "https://www.softwaretestinghelp.com/elk-stack-tutorial/#:~:text=A%20simple%20ELK%20stack%20architecture,stored%2C%20searched%2C%20and%20indexed. https://medium.com/dataseries/elk-stack-architecture-deep-dive-41168732f0e3 . | Built on top of Apache Lucene (it itself is a powerful search engine, all the power of Lucene easily expose to simple configuration and plugins, it handles human language synonyms, typo mistake) | NoSQL Datastore (like MongoDB) | Schema-free (no need to define a schema before adding data into Elasticsearch) | JSON Document (data in Elasticsearch is stored in form of JSON document) | RESTful APIs (Elasticsearch has powerful RESTful APIs that you can interact with cluster) | Node | Cluster | . ",
    "url": "http://localhost:4000/Resources#elk-stack-architecture",
    "relUrl": "/Resources#elk-stack-architecture"
  },"21": {
    "doc": "Resources",
    "title": "scaling",
    "content": "Vertical Scaling . Vertical scaling refers to increasing the processing power of a single server or cluster. Both relational and non-relational databases can scale up, but eventually, there will be a limit in terms of maximum processing power and throughput. Additionally, there are increased costs with scaling up to high-performing hardware, as costs do not scale linearly. Horizontal Scaling . Horizontal scaling, also known as scale-out, refers to bringing on additional nodes to share the load. This is difficult with relational databases due to the difficulty in spreading out related data across nodes. With non-relational databases, this is made simpler since collections are self-contained and not coupled relationally. This allows them to be distributed across nodes more simply, as queries do not have to “join” them together across nodes. ",
    "url": "http://localhost:4000/Resources#scaling",
    "relUrl": "/Resources#scaling"
  },"22": {
    "doc": "Resources",
    "title": "Elasticsearch scalability and resilience",
    "content": "Scalability and resilience: clusters, nodes, and shardsedit Elasticsearch is built to be always available and to scale with your needs. It does this by being distributed by nature. You can add servers (nodes) to a cluster to increase capacity and Elasticsearch automatically distributes your data and query load across all of the available nodes. No need to overhaul your application, Elasticsearch knows how to balance multi-node clusters to provide scale and high availability. The more nodes, the merrier. How does this work? Under the covers, an Elasticsearch index is really just a logical grouping of one or more physical shards, where each shard is actually a self-contained index. By distributing the documents in an index across multiple shards, and distributing those shards across multiple nodes, Elasticsearch can ensure redundancy, which both protects against hardware failures and increases query capacity as nodes are added to a cluster. As the cluster grows (or shrinks), Elasticsearch automatically migrates shards to rebalance the cluster. There are two types of shards: primaries and replicas. Each document in an index belongs to one primary shard. A replica shard is a copy of a primary shard. Replicas provide redundant copies of your data to protect against hardware failure and increase capacity to serve read requests like searching or retrieving a document. The number of primary shards in an index is fixed at the time that an index is created, but the number of replica shards can be changed at any time, without interrupting indexing or query operations. ",
    "url": "http://localhost:4000/Resources#elasticsearch-scalability-and-resilience",
    "relUrl": "/Resources#elasticsearch-scalability-and-resilience"
  },"23": {
    "doc": "Resources",
    "title": "elastic search search language",
    "content": "https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html . https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html . ",
    "url": "http://localhost:4000/Resources#elastic-search-search-language",
    "relUrl": "/Resources#elastic-search-search-language"
  },"24": {
    "doc": "Resources",
    "title": "jvm heap sizing",
    "content": "https://www.elastic.co/guide/en/cloud-enterprise/2.0/ece-heap.html#:~:text=Elasticsearch%20clusters%20and%20JVM%20Heap%20Size&amp;text=The%20ideal%20heap%20size%20is,container%20that%20hosts%20your%20cluster. https://www.elastic.co/guide/en/elasticsearch/reference/6.5/heap-size.html . ",
    "url": "http://localhost:4000/Resources#jvm-heap-sizing",
    "relUrl": "/Resources#jvm-heap-sizing"
  },"25": {
    "doc": "Resources",
    "title": "API and RESTful APIs in ELasticsearch",
    "content": "https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html . https://logz.io/blog/elasticsearch-api/ . https://hevodata.com/learn/elasticsearch-rest-api/ . ",
    "url": "http://localhost:4000/Resources#api-and-restful-apis-in-elasticsearch",
    "relUrl": "/Resources#api-and-restful-apis-in-elasticsearch"
  },"26": {
    "doc": "Resources",
    "title": "ETL",
    "content": "https://www.ibm.com/cloud/learn/etl#:~:text=ETL%2C%20which%20stands%20for%20extract,warehouse%20or%20other%20target%20system. https://en.wikipedia.org/wiki/Extract,_transform,_load . ",
    "url": "http://localhost:4000/Resources#etl",
    "relUrl": "/Resources#etl"
  },"27": {
    "doc": "Resources",
    "title": "Multitentants",
    "content": "https://www.techtarget.com/whatis/definition/multi-tenancy#:~:text=Multi%2Dtenancy%20is%20an%20architecture,customer%20is%20called%20a%20tenant. https://digitalguardian.com/blog/saas-single-tenant-vs-multi-tenant-whats-difference . https://www.sisense.com/glossary/multi-tenant/ . https://en.wikipedia.org/wiki/Multitenancy . ",
    "url": "http://localhost:4000/Resources#multitentants",
    "relUrl": "/Resources#multitentants"
  },"28": {
    "doc": "Resources",
    "title": "Security features SIEM",
    "content": "https://www.elastic.co/guide/en/kibana/current/xpack-siem.html#:~:text=Elastic%20Security%20combines%20SIEM%20threat,before%20damage%20and%20loss%20occur. ",
    "url": "http://localhost:4000/Resources#security-features-siem",
    "relUrl": "/Resources#security-features-siem"
  },"29": {
    "doc": "Resources",
    "title": "data tiers in elasticsearch",
    "content": "https://www.elastic.co/guide/en/elasticsearch/reference/current/data-tiers.html . https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html . ",
    "url": "http://localhost:4000/Resources#data-tiers-in-elasticsearch",
    "relUrl": "/Resources#data-tiers-in-elasticsearch"
  },"30": {
    "doc": "Resources",
    "title": "life cycle managment",
    "content": "https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html . ",
    "url": "http://localhost:4000/Resources#life-cycle-managment",
    "relUrl": "/Resources#life-cycle-managment"
  },"31": {
    "doc": "Resources",
    "title": "xdr elasticsearch",
    "content": "https://www.elastic.co/what-is/limitless-xdr . https://www.elastic.co/blog/elasticon-global-security . ",
    "url": "http://localhost:4000/Resources#xdr-elasticsearch",
    "relUrl": "/Resources#xdr-elasticsearch"
  },"32": {
    "doc": "Resources",
    "title": "Logstash parsing",
    "content": "https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html . https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html . https://www.tutorialspoint.com/logstash/logstash_parsing_the_logs.htm . ",
    "url": "http://localhost:4000/Resources#logstash-parsing",
    "relUrl": "/Resources#logstash-parsing"
  },"33": {
    "doc": "Resources",
    "title": "Resources",
    "content": " ",
    "url": "http://localhost:4000/Resources",
    "relUrl": "/Resources"
  },"34": {
    "doc": "Usage",
    "title": "Elasticsearch",
    "content": "List of Elasticsearch REST APIs . As said before elasticsearch provide an extensive RESTful APIs we gonna see some examples: . To perform an API request we can use different ways: . | Kibana console section | cURL requests | Any API platform(Post man…) | . ",
    "url": "http://localhost:4000/usage#elasticsearch",
    "relUrl": "/usage#elasticsearch"
  },"35": {
    "doc": "Usage",
    "title": "Index APIs",
    "content": "As its name implies, these API calls can be used to query indexed data for specific information. # Creates a new index. GET /PUT /my-index-000001 . # Returns information about one or more indices. For data streams, the API returns information about the stream’s backing indices. GET /my-index-000001 . ",
    "url": "http://localhost:4000/usage#index-apis",
    "relUrl": "/usage#index-apis"
  },"36": {
    "doc": "Usage",
    "title": "Search Api",
    "content": "Search APIs are used to search and aggregate data stored in Elasticsearch indices and data streams. # Returns search hits that match the query defined in the request. GET /my-index-000001/_search . ",
    "url": "http://localhost:4000/usage#search-api",
    "relUrl": "/usage#search-api"
  },"37": {
    "doc": "Usage",
    "title": "Kibana",
    "content": " ",
    "url": "http://localhost:4000/usage#kibana",
    "relUrl": "/usage#kibana"
  },"38": {
    "doc": "Usage",
    "title": "Visualize data",
    "content": "With the broad variety of visualization styles, Kibana allows you to create a visualization of your data in the Elasticsearch indices. ",
    "url": "http://localhost:4000/usage#visualize-data",
    "relUrl": "/usage#visualize-data"
  },"39": {
    "doc": "Usage",
    "title": "Dashboards",
    "content": "Dashboards in Kibana let you rapidly create views that pull together charts, maps, and filters to display the full picture of your Elasticsearch data. ",
    "url": "http://localhost:4000/usage#dashboards",
    "relUrl": "/usage#dashboards"
  },"40": {
    "doc": "Usage",
    "title": "Monitoring",
    "content": "We can use metricbeat to collect logs and metrics of our stack. The monitoring page of Kibana is usefull in multiple ways: . | You can visualize the data across Elastic Stack as it includes options to monitor the performance data for Elasticsearch, Kibana, Logstash as well as Beats in real-time | You can also analyze the past performance of these products | . ",
    "url": "http://localhost:4000/usage#monitoring",
    "relUrl": "/usage#monitoring"
  },"41": {
    "doc": "Usage",
    "title": "Security features SIEM",
    "content": "Elastic Security combines SIEM threat detection features with endpoint prevention and response capabilities in one solution. These analytical and protection capabilities, leveraged by the speed and extensibility of Elasticsearch, enable analysts to defend their organization from threats before damage and loss occur. Elastic Security provides the following security benefits and capabilities: . | A detection engine to identify attacks and system misconfigurations | A workspace for event triage and investigations | Interactive visualizations to investigate process relationships | Inbuilt case management with automated actions | Detection of signatureless attacks with prebuilt machine learning anomaly jobs and detection rules | . ",
    "url": "http://localhost:4000/usage#security-features-siem",
    "relUrl": "/usage#security-features-siem"
  },"42": {
    "doc": "Usage",
    "title": "Logstash",
    "content": " ",
    "url": "http://localhost:4000/usage#logstash",
    "relUrl": "/usage#logstash"
  },"43": {
    "doc": "Usage",
    "title": "Parsing log using logstash",
    "content": "Sometimes there is a perfect filter that can be used to parse your data, e.g. the json filter in case your logs are in JSON format. A lot of the time we do however need to parse logs in different types of text formats. The example we will use is a line of sysmon logs that looks as follows: . &lt;34&gt;1 2003-10-11T22:14:15.003Z mymachine.example.com su - ID47 - BOM'su root' failed for lonvick on /dev/pts/8 . ",
    "url": "http://localhost:4000/usage#parsing-log-using-logstash",
    "relUrl": "/usage#parsing-log-using-logstash"
  },"44": {
    "doc": "Usage",
    "title": "Usage",
    "content": " ",
    "url": "http://localhost:4000/usage",
    "relUrl": "/usage"
  }
}
